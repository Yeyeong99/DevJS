import os
import faiss
import pickle
import torch
import json
import numpy
import jsonlines
from dotenv import load_dotenv
from decouple import config
from pydantic import BaseModel
from typing import List

load_dotenv()

from groq import Groq
from transformers import AutoTokenizer, AutoModel


file_path = os.path.join("analyzes", "rag", "faiss", "rag_data.jsonl")    
data = []

with jsonlines.open(file_path) as reader:
    for obj in reader:
        data.append(obj)

texts = [item["text"] for item in data]

# ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ëŠ” í•œ ë²ˆë§Œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("BM-K/KoSimCSE-roberta")
model = AutoModel.from_pretrained("BM-K/KoSimCSE-roberta")

index_path = os.path.join("analyzes", "rag", "faiss", "faiss.index")    
metadata_path = os.path.join("analyzes", "rag", "faiss", "metadata.pkl")    

index = faiss.read_index(index_path)
with open(metadata_path, "rb") as f:
    metadatas = pickle.load(f)

# GROQ API
GROQ_API_KEY = config("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY)    # .envì— ë„£ì–´ì„œ ë¶ˆëŸ¬ì˜¤ê¸° : GROQ_API_KEY
MODEL_NAME = "gemma2-9b-it"

# ë²¡í„°í™” í•¨ìˆ˜ => í‚¤ì›Œë“œ ë²¡í„°í™” ì‹œ ì‚¬ìš©
def encode(text):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
    return cls_embedding.squeeze().cpu().numpy()



# í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
def build_total_prompt(question, coverletter, jd_keywords, retrieved_contexts):
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    contexts = "\n\n".join([
        f"- {c['text']}\ní”¼ë“œë°±: {c['metadata']['Feedback']}"
        for c in retrieved_contexts
    ])

    total_prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­ê³¼ ìê¸°ì†Œê°œì„œ ë‹µë³€, ìê¸°ì†Œê°œì„œ ë‹µë³€ì—ì„œ ê°•ì¡°í•˜ê³ ì í•˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤.
    ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í”¼ë“œë°±ê³¼ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ ì˜ˆì‹œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­:
    {question.strip()}

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€:
    {coverletter.strip()}

    ì‚¬ìš©ìì˜ ì£¼ìš” í‚¤ì›Œë“œ:
    {jd_keywords.strip()}

    ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±(ì¢…í•© í”¼ë“œë°± ì°¸ê³ ìš©):
    {contexts.strip()}

    ğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì£¼ì„¸ìš”:
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œì— ëŒ€í•´ì„œë§Œ ì¢…í•© í”¼ë“œë°±ì„ ì‘ì„±í•  ë•Œ, ì‚¬ìš©ìì˜ ì£¼ìš” í‚¤ì›Œë“œê°€ ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ì— ê´€í•œ íŒë‹¨ê³¼, ì˜í•œ ì ê³¼ ë³´ì™„í•´ì•¼í•  ì ì„ í¬í•¨í•´ `feedback`ì— ì‘ì„±í•´ì£¼ì„¸ìš”.
    ì´ë•Œ ì¢…í•© í”¼ë“œë°±ì˜ ì˜í•œ ì ê³¼ ë³´ì™„ì´ í•„ìš”í•œ ì ì€ 1. ìŠ¤í† ë¦¬í…”ë§ ì¸¡ë©´ 2. ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ê°™ì€ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ë“¤ì–´ ì‘ì„±í•´ì•¼í•©ë‹ˆë‹¤.
    `ai_feedback`ì—ëŠ” ì¢…í•© í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì´ë•Œ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œì— ë³´ì™„ì„ ìœ„í•œ ì˜ˆì‹œ ë¬¸ì¥ì´ í¬í•¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€ê³¼ ìœ ì‚¬í•œ ì˜ˆì‹œë¥¼ ë“¤ë©´ ì•ˆë©ë‹ˆë‹¤.
    ğŸ“Œ ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤. âœ…
        {{
            'feedback': '...',
            'ai_feedback': '...', 
        }}
    """
    return total_prompt

class TotalFeedback(BaseModel):
    feedback: str
    ai_feedback: str

def get_llm_total_feedback(total_prompt) -> TotalFeedback:
    response = client.chat.completions.create(
        model = MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": 
                """
                    ë‹¹ì‹ ì€ IT ë¶„ì•¼ë¡œ ì§„ë¡œë¥¼ ì •í•œ ì·¨ì—…ì¤€ë¹„ìƒë“¤ì˜ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
                    ì‘ë‹µì€ ë‹¤ìŒ JSON êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
                    {
                        "feedback": "ìê¸°ì†Œê°œì„œì— ëŒ€í•œ ì „ë°˜ì ì¸ í”¼ë“œë°±",
                        "ai_feedback": "ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ ì˜ˆì‹œ"
                    }
                """
            },
            {
                "role": "user", 
                "content": total_prompt
            }
        ],
        temperature=0.5,
        stream=False,
        response_format={"type": "json_object"},        
    )
    
    try:
        raw_json = json.loads(response.choices[0].message.content)
        # Validate that the required fields exist
        if "feedback" not in raw_json or "ai_feedback" not in raw_json:
            raise ValueError("Missing required fields in JSON response")
            
        return TotalFeedback(
            feedback=raw_json["feedback"],
            ai_feedback=raw_json["ai_feedback"]
        )
    except (json.JSONDecodeError, ValueError) as e:
        # Log the error and the raw response for debugging
        print(f"Error parsing LLM response: {e}")
        print(f"Raw response: {response.choices[0].message.content}")
        
        # Provide a fallback response
        return TotalFeedback(
            feedback="ìê¸°ì†Œê°œì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            ai_feedback="í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )