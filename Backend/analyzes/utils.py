import os
import faiss
import pickle
import torch
import json
import numpy
import jsonlines
from dotenv import load_dotenv
from pydantic import BaseModel
from typing import List

load_dotenv()

from groq import Groq
from transformers import AutoTokenizer, AutoModel


file_path = os.path.join("analyzes", "rag", "faiss", "rag_data.jsonl")    
data = []

with jsonlines.open(file_path) as reader:
    for obj in reader:
        data.append(obj)

texts = [item["text"] for item in data]

# ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ëŠ” í•œ ë²ˆë§Œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("BM-K/KoSimCSE-roberta")
model = AutoModel.from_pretrained("BM-K/KoSimCSE-roberta")

index_path = os.path.join("analyzes", "rag", "faiss", "faiss.index")    
metadata_path = os.path.join("analyzes", "rag", "faiss", "metadata.pkl")    

index = faiss.read_index(index_path)
with open(metadata_path, "rb") as f:
    metadatas = pickle.load(f)

# GROQ API
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
client = Groq(api_key=GROQ_API_KEY)    # .envì— ë„£ì–´ì„œ ë¶ˆëŸ¬ì˜¤ê¸° : GROQ_API_KEY
MODEL_NAME = "gemma2-9b-it"

# ë²¡í„°í™” í•¨ìˆ˜ => í‚¤ì›Œë“œ ë²¡í„°í™” ì‹œ ì‚¬ìš©
def encode(text):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
    return cls_embedding.squeeze().cpu().numpy()



# í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
def build_total_prompt(question, coverletter, jd_keywords, retrieved_contexts):
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    contexts = "\n\n".join([
        f"- {c['text']}\ní”¼ë“œë°±: {c['metadata']['Feedback']}"
        for c in retrieved_contexts
    ])

    total_prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­ê³¼ ìê¸°ì†Œê°œì„œ ë‹µë³€, ìê¸°ì†Œê°œì„œ ë‹µë³€ì„ í†µí•´ ê°•ì¡°í•˜ê³ ì í•˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤.
    ì²¨ì‚­ ì‹œ ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­:
    {question.strip()}

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€:
    {coverletter.strip()}

    ì‚¬ìš©ìì˜ ì£¼ìš” í‚¤ì›Œë“œ:
    {jd_keywords.strip()}

    ì²¨ì‚­ ì‹œ ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±:
    {contexts.strip()}

    ğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì£¼ì„¸ìš”:
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œì— ëŒ€í•´ì„œë§Œ ì¢…í•© í”¼ë“œë°±ì„ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ë“¤ì–´ ì‘ì„±í•˜ê³ , í•„ìš”í•  ê²½ìš° êµ¬ì²´ì ì¸ ë¬¸ì¥ ê°œì„ ì˜ ì˜ˆì‹œë¥¼ ì§ì ‘ ë§Œë“¤ì–´ `feedback` í•­ëª©ì— ì‘ì„±í•´ì£¼ì„¸ìš”.
    í”¼ë“œë°±ì—ëŠ” í‚¤ì›Œë“œê°€ ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ì— ê´€í•œ íŒë‹¨ê³¼, ì˜í•œ ì ê³¼ ë³´ì™„í•´ì•¼í•  ì ì´ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì¥ ê°œì„ ì˜ ì˜ˆì‹œë¥¼ ë“¤ ë•Œ, ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€ê³¼ ìœ ì‚¬í•œ ì˜ˆì‹œë¥¼ ë“¤ë©´ ì•ˆë©ë‹ˆë‹¤.
    ğŸ“Œ ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤. âœ…
        {{
            'feedback': '...'
        }}
    """
    return total_prompt

def build_sentence_prompt(coverletter, total_feedback):
    sentence_prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€ê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì¢…í•© í”¼ë“œë°±ì…ë‹ˆë‹¤.
    ì¢…í•© í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ê° ë¬¸ì¥ì— ëŒ€í•´ ì²¨ì‚­í•œ í›„, ì²¨ì‚­ ì „ê³¼ í›„ ë¬¸ì¥ì„ ê°ê°ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

    ìê¸°ì†Œê°œì„œ ë‹µë³€:
    {coverletter.strip()}

    ì¢…í•© í”¼ë“œë°±:
    {total_feedback.strip()}


    ğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì£¼ì„¸ìš”:
    1. ìê¸°ì†Œê°œì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ ë¬¸ì¥ë“¤ì„ `before_feedback`ì— ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    2. ìê¸°ì†Œê°œì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ ë¬¸ì¥ë“¤ì„ ì²¨ì‚­í•˜ëŠ”ë°, ì¢…í•© í”¼ë“œë°±ì„ ë³´ê³  êµ¬ì²´ì ì¸ ì˜ˆì‹œê°€ í•„ìš”í•  ê²½ìš° ì§„ì§œ ì˜ˆì‹œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê·¸ ê²°ê³¼ë¥¼ `after_feedback`ì— ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    3. JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ë©°, í•­ëª©ì€ ë°˜ë“œì‹œ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¦…ë‹ˆë‹¤.
    ğŸ“Œ ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤. âœ…
        {{
            'before_feedback': ['...'],
            'after_feedback': ['...']
        }}
    âš ï¸ ì¤‘ê°„ì— 'properties' ê°™ì€ í‚¤ ì—†ì´ flatí•œ JSON í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤!
    4. ê° ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ëŠ” ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    return sentence_prompt

class TotalFeedback(BaseModel):
    feedback: str
    
class Feedback(BaseModel):
    before_feedback: List[str]
    after_feedback: List[str]

def get_llm_total_feedback(total_prompt) -> TotalFeedback:
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ IT ë¶„ì•¼ë¡œ ì§„ë¡œë¥¼ ì •í•œ ì·¨ì—…ì¤€ë¹„ìƒë“¤ì˜ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
                # json ìŠ¤í‚¤ë§ˆ ì „ë‹¬
                f" The JSON object must use the schema: {json.dumps(TotalFeedback.model_json_schema(), indent=2)}",
            },
            {
                "role": "user", 
                "content": total_prompt
            }
        ],
        temperature=0.5,
        stream=False,
        # Jsonìœ¼ë¡œ í¬ë§· ì§€ì •
        response_format={"type": "json_object"},        
    )
    raw_json = json.loads(response.choices[0].message.content)
    return raw_json

def get_sentence_feedback(prompt) -> Feedback:
    response = client.chat.completions.create(
        messages=[
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ IT ë¶„ì•¼ë¡œ ì§„ë¡œë¥¼ ì •í•œ ì·¨ì—…ì¤€ë¹„ìƒë“¤ì˜ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
                # json ìŠ¤í‚¤ë§ˆ ì „ë‹¬
                f" The JSON object must use the schema: {json.dumps(Feedback.model_json_schema(), indent=2)}",
            },
            {
                "role": "user",
                "content": prompt
            },
        ],
        model=MODEL_NAME,
        temperature=0,
        stream=False,
        # Jsonìœ¼ë¡œ í¬ë§· ì§€ì •
        response_format={"type": "json_object"},
    )

    raw_json = json.loads(response.choices[0].message.content)

    return raw_json