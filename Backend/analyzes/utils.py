import os
import faiss
import pickle
import torch
import json
import numpy
import jsonlines
from dotenv import load_dotenv
from pydantic import BaseModel
from typing import List

load_dotenv()

from groq import Groq
from transformers import AutoTokenizer, AutoModel


file_path = os.path.join("analyzes", "rag", "faiss", "rag_data.jsonl")    
data = []

with jsonlines.open(file_path) as reader:
    for obj in reader:
        data.append(obj)

texts = [item["text"] for item in data]

# ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ëŠ” í•œ ë²ˆë§Œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("BM-K/KoSimCSE-roberta")
model = AutoModel.from_pretrained("BM-K/KoSimCSE-roberta")

index_path = os.path.join("analyzes", "rag", "faiss", "faiss.index")    
metadata_path = os.path.join("analyzes", "rag", "faiss", "metadata.pkl")    

index = faiss.read_index(index_path)
with open(metadata_path, "rb") as f:
    metadatas = pickle.load(f)

# GROQ API
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
client = Groq(api_key=GROQ_API_KEY)    # .envì— ë„£ì–´ì„œ ë¶ˆëŸ¬ì˜¤ê¸° : GROQ_API_KEY
MODEL_NAME = "gemma2-9b-it"

# ë²¡í„°í™” í•¨ìˆ˜ => í‚¤ì›Œë“œ ë²¡í„°í™” ì‹œ ì‚¬ìš©
def encode(text):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
    return cls_embedding.squeeze().cpu().numpy()



# í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
def build_total_prompt(question, coverletter, jd_keywords, retrieved_contexts):
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    contexts = "\n\n".join([
        f"- {c['text']}\ní”¼ë“œë°±: {c['metadata']['Feedback']}"
        for c in retrieved_contexts
    ])

    total_prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­ê³¼ ìê¸°ì†Œê°œì„œ ë‹µë³€, ìê¸°ì†Œê°œì„œ ë‹µë³€ì—ì„œ ê°•ì¡°í•˜ê³ ì í•˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤.
    ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í”¼ë“œë°±ê³¼ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ ì˜ˆì‹œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­:
    {question.strip()}

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€:
    {coverletter.strip()}

    ì‚¬ìš©ìì˜ ì£¼ìš” í‚¤ì›Œë“œ:
    {jd_keywords.strip()}

    ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±(ì¢…í•© í”¼ë“œë°± ì°¸ê³ ìš©):
    {contexts.strip()}

    ğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì£¼ì„¸ìš”:
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œì— ëŒ€í•´ì„œë§Œ ì¢…í•© í”¼ë“œë°±ì„ ì‘ì„±í•  ë•Œ, ì‚¬ìš©ìì˜ ì£¼ìš” í‚¤ì›Œë“œê°€ ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ì— ê´€í•œ íŒë‹¨ê³¼, ì˜í•œ ì ê³¼ ë³´ì™„í•´ì•¼í•  ì ì„ í¬í•¨í•´ `feedback`ì— ì‘ì„±í•´ì£¼ì„¸ìš”.
    ì´ë•Œ ì¢…í•© í”¼ë“œë°±ì˜ ì˜í•œ ì ê³¼ ë³´ì™„ì´ í•„ìš”í•œ ì ì€ 1. ìŠ¤í† ë¦¬í…”ë§ ì¸¡ë©´ 2. ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ê°™ì€ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ë“¤ì–´ ì‘ì„±í•´ì•¼í•©ë‹ˆë‹¤.
    `ai_feedback`ì—ëŠ” ì¢…í•© í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì´ë•Œ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œì— ë³´ì™„ì„ ìœ„í•œ ì˜ˆì‹œ ë¬¸ì¥ì´ í¬í•¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€ê³¼ ìœ ì‚¬í•œ ì˜ˆì‹œë¥¼ ë“¤ë©´ ì•ˆë©ë‹ˆë‹¤.
    ğŸ“Œ ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤. âœ…
        {{
            'feedback': '...',
            'ai_feedback': '...', 
        }}
    """
    return total_prompt

class TotalFeedback(BaseModel):
    feedback: str
    ai_feedback: str

def get_llm_total_feedback(total_prompt) -> TotalFeedback:
    response = client.chat.completions.create(
        model = MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ IT ë¶„ì•¼ë¡œ ì§„ë¡œë¥¼ ì •í•œ ì·¨ì—…ì¤€ë¹„ìƒë“¤ì˜ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
                # json ìŠ¤í‚¤ë§ˆ ì „ë‹¬
                f" The JSON object must use the schema: {json.dumps(TotalFeedback.model_json_schema(), indent=2)}",
            },
            {
                "role": "user", 
                "content": total_prompt
            }
        ],
        temperature=0.5,
        stream=False,
        # Jsonìœ¼ë¡œ í¬ë§· ì§€ì •
        response_format={"type": "json_object"},        
    )
    raw_json = json.loads(response.choices[0].message.content)
    return raw_json