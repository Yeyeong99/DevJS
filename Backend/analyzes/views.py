import os
import faiss
import pickle
import torch
import json
import numpy
import jsonlines
from dotenv import load_dotenv
from decouple import config
from pydantic import BaseModel
from typing import List

load_dotenv()

from groq import Groq
from transformers import AutoTokenizer, AutoModel


file_path = os.path.join("analyzes", "rag", "faiss", "rag_data.jsonl")    
data = []

with jsonlines.open(file_path) as reader:
    for obj in reader:
        data.append(obj)

texts = [item["text"] for item in data]

# ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ëŠ” í•œ ë²ˆë§Œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("BM-K/KoSimCSE-roberta")
model = AutoModel.from_pretrained("BM-K/KoSimCSE-roberta")

index_path = os.path.join("analyzes", "rag", "faiss", "faiss.index")    
metadata_path = os.path.join("analyzes", "rag", "faiss", "metadata.pkl")    

index = faiss.read_index(index_path)
with open(metadata_path, "rb") as f:
    metadatas = pickle.load(f)

# GROQ API
GROQ_API_KEY = config("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY)    # .envì— ë„£ì–´ì„œ ë¶ˆëŸ¬ì˜¤ê¸° : GROQ_API_KEY
MODEL_NAME = "gemma2-9b-it"

# ë²¡í„°í™” í•¨ìˆ˜ => í‚¤ì›Œë“œ ë²¡í„°í™” ì‹œ ì‚¬ìš©
def encode(text):
    inputs = tokenizer([text], return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
    return cls_embedding.squeeze().cpu().numpy()

def build_score_propmt(coverletter, retrieved_contexts):
    contexts = "\n\n".join([
        f"- {c['metadata']['SelfIntroduction']}\nì ìˆ˜: {c['metadata']['JDReflectRate']}"
        for c in retrieved_contexts
    ])

    check_score_propmt = f"""
    ë‹¤ìŒì— ì˜¤ëŠ” ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œë¥¼ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ 1ì—ì„œ 5ì  ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
    í‰ê°€ë¥¼ í•  ë•Œ ì°¸ê³ ìš© ìê¸°ì†Œê°œì„œì™€ ì ìˆ˜ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.

    í‰ê°€ ê¸°ì¤€:
    - 1: JDì˜ í‚¤ì›Œë“œ ì–¸ê¸‰ë§Œ ë¨. ê´€ë ¨í•œ ìì‹ ì˜ ê²½í—˜ì´ ì–¸ê¸‰ë˜ì§€ ì•ŠìŒ.
    - 2: JDì˜ í‚¤ì›Œë“œ ì–¸ê¸‰. ìì‹ ì˜ ê²½í—˜ ìˆì§€ë§Œ êµ¬ì²´ì„± ì—†ìŒ. ì–¸ê¸‰ì— ëŒ€í•œ êµ¬ì²´ì„±ë„ ì—†ìŒ
    - 3: JDì˜ í‚¤ì›Œë“œ ì–¸ê¸‰. ìì‹ ì˜ ê²½í—˜ ìˆê³  êµ¬ì²´ì„± ìˆìŒ. ë‹¤ë§Œ ë‹¨ìˆœ ë‚˜ì—´ì‹
    - 4: JDì˜ í‚¤ì›Œë“œë‘ ê´€ë ¨í•œ ê²½í—˜, ì„±ì·¨ë¥¼ ì˜ ì–¸ê¸‰í•¨. ìŠ¤í† ë¦¬í…”ë§ì´ ì˜ ë˜ì–´ ìˆìŒ.
    - 5: JDì˜ í‚¤ì›Œë“œ ì–¸ê¸‰í•¨. ìŠ¤í† ë¦¬í…”ë§ ì¢‹ìŒ. ì•ìœ¼ë¡œ ì‹¤ë¬´ì— ì–´ë–»ê²Œ ë°˜ì˜í•  ê±´ì§€ë„ ì˜ ë°˜ì˜ë¨

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ:
    {coverletter.strip()}
    
    ì°¸ê³ ìš© ìê¸°ì†Œê°œì„œ:
    {contexts.strip()}
    """

    return check_score_propmt

class FeedbackScore(BaseModel):
    score: int

def get_llm_score(check_score_propmt) -> FeedbackScore:
    response = client.chat.completions.create(
        model = MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": 
                """
                    ë‹¹ì‹ ì€ IT ë¶„ì•¼ë¡œ ì§„ë¡œë¥¼ ì •í•œ ì·¨ì—…ì¤€ë¹„ìƒë“¤ì˜ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ë°˜ë“œì‹œ ì •ìˆ˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
                    ì‘ë‹µì€ ë‹¤ìŒ JSON êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
                    {
                        "score": 1
                    }
                """
            },
            {
                "role": "user", 
                "content": check_score_propmt
            }
        ],
        temperature=0.5,
        stream=False,
        response_format={"type": "json_object"},        
    )
    
    try:
        raw_json = json.loads(response.choices[0].message.content)
        # Validate that the required fields exist
        if "score" not in raw_json:
            raise ValueError("Missing required fields in JSON response")
            
        return FeedbackScore(
            score=raw_json["score"],
        )
    
    except (json.JSONDecodeError, ValueError) as e:
        # Log the error and the raw response for debugging
        print(f"Error parsing LLM response: {e}")
        print(f"Raw response: {response.choices[0].message.content}")
        
        # Provide a fallback response
        return FeedbackScore(
            score=0,
        )

# í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
def build_total_prompt(question, coverletter, score, retrieved_contexts):
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    contexts = "\n\n".join([
        f"- {c['metadata']['SelfIntroduction']}\nì ìˆ˜: {c['metadata']['JDReflectRate']}\ní”¼ë“œë°±: {c['metadata']['Feedback']}"
        for c in retrieved_contexts
    ])

    total_prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­ê³¼ ìê¸°ì†Œê°œì„œ ë‹µë³€, ìê¸°ì†Œê°œì„œ ë‹µë³€ì—ì„œ ê°•ì¡°í•˜ê³ ì í•˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤.
    ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í”¼ë“œë°±ê³¼ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ ì˜ˆì‹œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë¬¸í•­:
    {question.strip()}

    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€:
    {coverletter.strip()}

    ìê¸°ì†Œê°œì„œì˜ ì ìˆ˜:
    {score.strip()}

    ì°¸ê³ ìš© ìì†Œì„œ ë° í”¼ë“œë°±(ì¢…í•© í”¼ë“œë°± ì°¸ê³ ìš©):
    {contexts.strip()}

    ğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì£¼ì„¸ìš”:
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œì— ëŒ€í•´ì„œë§Œ ì¢…í•© í”¼ë“œë°±ì„ ì‘ì„±í•  ë•Œ, ì‚¬ìš©ìì˜ ì£¼ìš” í‚¤ì›Œë“œê°€ ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ì— ê´€í•œ íŒë‹¨ê³¼, ì˜í•œ ì ê³¼ ë³´ì™„í•´ì•¼í•  ì ì„ í¬í•¨í•´ `feedback`ì— ì‘ì„±í•´ì£¼ì„¸ìš”.
    ì´ë•Œ ì¢…í•© í”¼ë“œë°±ì˜ ì˜í•œ ì ê³¼ ë³´ì™„ì´ í•„ìš”í•œ ì ì€ 1. ìŠ¤í† ë¦¬í…”ë§ ì¸¡ë©´ 2. ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ê°™ì€ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ë“¤ì–´ ì‘ì„±í•´ì•¼í•©ë‹ˆë‹¤.
    `ai_feedback`ì—ëŠ” ì¢…í•© í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì´ë•Œ ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œì— ë³´ì™„ì„ ìœ„í•œ ì˜ˆì‹œ ë¬¸ì¥ì´ í¬í•¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‹µë³€ê³¼ ìœ ì‚¬í•œ ì˜ˆì‹œë¥¼ ë“¤ë©´ ì•ˆë©ë‹ˆë‹¤.
    [êµ¬ì²´ì ì¸ ì˜ˆì‹œ], [êµ¬ì²´ì ì¸ ì‚¬ë¡€] ì´ëŸ° ì‹ìœ¼ë¡œ ëª¨í˜¸í•œ í‘œí˜„ì„ ì“°ì§€ ë§ê³  ì‹¤ì§ˆì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•œ ì˜ˆì‹œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”. 
    ğŸ“Œ ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤. âœ…
        {{
            'feedback': '...',
            'ai_feedback': '...', 
        }}
    """
    return total_prompt

class TotalFeedback(BaseModel):
    feedback: str
    ai_feedback: str

def get_llm_total_feedback(total_prompt) -> TotalFeedback:
    response = client.chat.completions.create(
        model = MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": 
                """
                    ë‹¹ì‹ ì€ IT ë¶„ì•¼ë¡œ ì§„ë¡œë¥¼ ì •í•œ ì·¨ì—…ì¤€ë¹„ìƒë“¤ì˜ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
                    ì‘ë‹µì€ ë‹¤ìŒ JSON êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
                    {
                        "feedback": "ìê¸°ì†Œê°œì„œì— ëŒ€í•œ ì „ë°˜ì ì¸ í”¼ë“œë°±",
                        "ai_feedback": "ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ ì˜ˆì‹œ"
                    }
                """
            },
            {
                "role": "user", 
                "content": total_prompt
            }
        ],
        temperature=0.2,
        stream=False,
        response_format={"type": "json_object"},        
    )
    
    try:
        raw_json = json.loads(response.choices[0].message.content)
        # Validate that the required fields exist
        if "feedback" not in raw_json or "ai_feedback" not in raw_json:
            raise ValueError("Missing required fields in JSON response")
            
        return TotalFeedback(
            feedback=raw_json["feedback"],
            ai_feedback=raw_json["ai_feedback"]
        )
    except (json.JSONDecodeError, ValueError) as e:
        # Log the error and the raw response for debugging
        print(f"Error parsing LLM response: {e}")
        print(f"Raw response: {response.choices[0].message.content}")
        
        # Provide a fallback response
        return TotalFeedback(
            feedback="ìê¸°ì†Œê°œì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            ai_feedback="í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )